# Team 4 Report

## Project description

## Quantitative evaluation 
To evaluate the performance of the classifiers, we have selected a set of four key evaluation metrics, these metrics include Accuracy, Precision, Recall and F1 Score. These metrics help us understand how well our classifiers are performing on various aspects. See below a short motivation per metrics. 

Accuracy: this is a straightforward measure of overall correctness. With measuring the accuracy you answer the question of how many predictions our classifiers got right, out of all the predictions made. We aim for a high accuracy, as that is desirable because it indicates a high proportion of correct predictions. 

Precision: building in accuracy we also decided to measure the level of precision. Calculating this score gives you an insight in the proportion of true positive predictions out of all the positive predictions our classifiers have made. A high precision score is valuable to us as we want to minimize the false positive error. 

Recall: with measuring recall we capture our classifiers' ability to identify all the instances right. To be precise, with recall we calculate the exact proportion of true positive predictions among all the actual positive instances that occur in our dataset. This is important if we want to minimize the false negatives. 

F1 Score: the last metrics we have chosen is the F1 Score. The F1 score takes into account both the false positives and false negatives, reaching a balance. This we measure because we aim to achieve a balance between making accurate positive predictions and capturing all relevant positive instances. 

## Error analysis
*Are there specific dialog acts that are more difficult to classify? Are there particular utterances that are hard to classify (for all systems)? And why?*
...

## Difficult cases
*Come up with two types of ‘difficult instances’, for example utterances that are not fluent (e.g. due to speech recognition issues) or the presence of negation (I don’t want an expensive restaurant). For each case, create test instances and evaluate how your systems perform on these cases.*
...

## System comparison
*How do the systems compare against the baselines, and against each other? What is the influence of deduplication? Which one would you choose for your dialog system?*
...
